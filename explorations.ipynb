{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da0b249",
   "metadata": {},
   "source": [
    "## Explore knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f77d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "vector_db = lancedb.connect(uri = \"knowledge_base\")\n",
    "vector_db.table_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c993276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceTable(name='articles', version=107, _conn=LanceDBConnection(uri='c:\\\\Users\\\\eriku\\\\Desktop\\\\DataenginerSTI2024-2026\\\\Github\\\\Youtube_ragbot\\\\knowledge_base'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5e9c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An introduction to the vector database LanceDB</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>An introduction to the vector database LanceDB</td>\n",
       "      <td># An introduction to the vector database Lance...</td>\n",
       "      <td>[-0.038686633, 0.0036908067, 0.02178414, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>API trafiklab (1)</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>API trafiklab (1)</td>\n",
       "      <td># API trafiklab\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[-0.0066460855, -0.0048297336, 0.014259387, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>API trafiklab</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>API trafiklab</td>\n",
       "      <td># API trafiklab\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[-0.0066460855, -0.0048297336, 0.014259387, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azure static web app deploy react app</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Azure static web app deploy react app</td>\n",
       "      <td># Azure static web app deploy react app\\n\\n[00...</td>\n",
       "      <td>[-0.012295628, 0.015832268, 0.025340993, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chat with your excel data - xlwings lite (1)</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Chat with your excel data - xlwings lite (1)</td>\n",
       "      <td># Chat with your excel data - xlwings lite\\n\\n...</td>\n",
       "      <td>[-0.0071265996, 0.020476552, 0.017235534, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chat with your excel data - xlwings lite</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Chat with your excel data - xlwings lite</td>\n",
       "      <td># Chat with your excel data - xlwings lite\\n\\n...</td>\n",
       "      <td>[-0.0071265996, 0.020476552, 0.017235534, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Course structure for Azure two weeks course</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Course structure for Azure two weeks course</td>\n",
       "      <td># Course structure for Azure two weeks course\\...</td>\n",
       "      <td>[0.0015313567, 0.012794174, 0.016358217, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data platform course structure</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Data platform course structure</td>\n",
       "      <td># Data platform course structure\\n\\n[00:00:00]...</td>\n",
       "      <td>[0.010435624, -0.0016020014, 0.011163727, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data processing course  structure</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>data processing course  structure</td>\n",
       "      <td># data processing course structure\\n\\n**Kokchu...</td>\n",
       "      <td>[0.0032914313, 0.01059015, 0.009196502, -0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data storytelling</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>data storytelling</td>\n",
       "      <td># data storytelling\\n\\n[00:00:00] Hello and we...</td>\n",
       "      <td>[-0.011316549, 0.016874935, 0.012392512, -0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dbt modeling snowflake</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>dbt modeling snowflake</td>\n",
       "      <td># dbt modeling snowflake\\n\\n**Kokchun Giang:**...</td>\n",
       "      <td>[-0.010636117, 0.01212832, 0.029030465, -0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>docker setup windows</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>docker setup windows</td>\n",
       "      <td># docker setup windows\\n\\n[00:00:00] Hello and...</td>\n",
       "      <td>[-0.0034482135, 0.010729573, 0.016939094, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FastAPI and scikit-learn API connect to stream...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>FastAPI and scikit-learn API connect to stream...</td>\n",
       "      <td># FastAPI and scikit-learn API connect to stre...</td>\n",
       "      <td>[-0.017560659, 0.009985835, 0.01749353, -0.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fastapi CRUD app</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Fastapi CRUD app</td>\n",
       "      <td># Fastapi CRUD app\\n\\n**Kokchun Giang:** [00:0...</td>\n",
       "      <td>[-0.012556567, -0.01593715, 0.022671303, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hands on regularization</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Hands on regularization</td>\n",
       "      <td># Hands on regularization\\n\\n[00:00:00] Hello ...</td>\n",
       "      <td>[-0.009709013, 6.357031e-05, 0.023912106, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How does LLM work_</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>How does LLM work_</td>\n",
       "      <td># How does LLM work?\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.027908303, 0.017137818, 0.023856219, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logistic regression hands on with scikit learn</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Logistic regression hands on with scikit learn</td>\n",
       "      <td># Logistic regression hands on with scikit lea...</td>\n",
       "      <td>[-0.0066825696, 0.002244388, 0.011106265, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Logistic regression theory</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Logistic regression theory</td>\n",
       "      <td># Logistic regression theory\\n\\n[00:00:00] Hel...</td>\n",
       "      <td>[-0.008146983, -0.00016716555, 0.011266027, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Modern data stack - deploy dockerized dashboar...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Modern data stack - deploy dockerized dashboar...</td>\n",
       "      <td># Modern data stack - deploy dockerized dashbo...</td>\n",
       "      <td>[-0.0075815883, 0.02116889, 0.027429571, -0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Modern data stack - dockerize your data pipeline</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Modern data stack - dockerize your data pipeline</td>\n",
       "      <td># Modern data stack - dockerize your data pipe...</td>\n",
       "      <td>[0.0027987233, 0.014228618, 0.027441906, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Modern data stack - using dlt to extract and l...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Modern data stack - using dlt to extract and l...</td>\n",
       "      <td># Modern data stack - using dlt to extract and...</td>\n",
       "      <td>[-0.00826649, 0.019363934, 0.016988434, -0.080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Packaging in python</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Packaging in python</td>\n",
       "      <td># Packaging in python\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.0150246415, 0.0042777252, 0.038052212, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pandas_read_excel</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>pandas_read_excel</td>\n",
       "      <td># pandas\\_read\\_excel\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.0038475876, 0.0054737586, 0.024301918, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>postgres sink</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>postgres sink</td>\n",
       "      <td># postgres sink\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[0.007228276, 0.021961471, 0.031665433, -0.088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Pydantic fundamentals</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Pydantic fundamentals</td>\n",
       "      <td># Pydantic fundamentals\\n\\n[00:00:00] Hello an...</td>\n",
       "      <td>[-0.017777685, -0.01204192, 0.02006065, -0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pydantic with gemini to structure output in a ...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Pydantic with gemini to structure output in a ...</td>\n",
       "      <td># Pydantic with gemini to structure output in ...</td>\n",
       "      <td>[-0.01512796, -0.003907627, 0.015176425, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pydanticAI chatbot</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>pydanticAI chatbot</td>\n",
       "      <td># pydanticAI chatbot\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.012143856, -0.001205422, 0.013563879, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PydanticAI fundamentals - outputting structure...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>PydanticAI fundamentals - outputting structure...</td>\n",
       "      <td># PydanticAI fundamentals - outputting structu...</td>\n",
       "      <td>[-0.033087507, 0.0036856267, 0.02514256, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pytest unit testing</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>pytest unit testing</td>\n",
       "      <td># pytest unit testing\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.014343338, 0.0007622975, 0.030120881, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Python fundamentals</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Python fundamentals</td>\n",
       "      <td># Python fundamentals\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.011976539, 0.0025688808, 0.015519834, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>python intro</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>python intro</td>\n",
       "      <td># python intro\\n\\n[00:00:00] Hello and welcome...</td>\n",
       "      <td>[-0.0110368235, -0.0020397531, 0.012955041, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Python_oop_1</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Python_oop_1</td>\n",
       "      <td># Python\\_oop\\_1\\n\\n**kokchun:** [00:00:00] He...</td>\n",
       "      <td>[-0.008043373, -0.011165032, 0.031197485, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Serving PydanticAI Gemini Model with FastAPI T...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Serving PydanticAI Gemini Model with FastAPI T...</td>\n",
       "      <td># Serving PydanticAI Gemini Model with FastAPI...</td>\n",
       "      <td>[-0.018388461, -0.006911759, 0.015190295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Serving PydanticAI Gemini Model with FastAPI T...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Serving PydanticAI Gemini Model with FastAPI T...</td>\n",
       "      <td># Serving PydanticAI Gemini Model with FastAPI...</td>\n",
       "      <td>[-0.018388461, -0.006911759, 0.015190295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SQL analytics course with DuckDB - course stru...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - course stru...</td>\n",
       "      <td># SQL analytics course with DuckDB - course st...</td>\n",
       "      <td>[0.0023245383, -0.0029253534, 0.0036489798, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SQL analytics course with DuckDB - CRUD operat...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - CRUD operat...</td>\n",
       "      <td># SQL analytics course with DuckDB - CRUD oper...</td>\n",
       "      <td>[-0.018357037, -0.0009783215, 0.025988214, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SQL analytics course with DuckDB - dlt to load...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - dlt to load...</td>\n",
       "      <td># SQL analytics course with DuckDB - dlt to lo...</td>\n",
       "      <td>[-0.0066205882, -0.0075128363, 0.019882608, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SQL analytics course with DuckDB - joins concepts</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - joins concepts</td>\n",
       "      <td># SQL analytics course with DuckDB - joins con...</td>\n",
       "      <td>[-0.027722066, -0.016966412, 0.012043696, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>SQL analytics course with DuckDB - joins with ...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - joins with ...</td>\n",
       "      <td># SQL analytics course with DuckDB - joins wit...</td>\n",
       "      <td>[-0.022949534, -0.022863599, 0.013395227, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SQL analytics course with DuckDB - pandas and ...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - pandas and ...</td>\n",
       "      <td># SQL analytics course with DuckDB - pandas an...</td>\n",
       "      <td>[-0.009578243, -0.011996515, 0.012965736, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SQL analytics course with DuckDB - pandas and ...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - pandas and ...</td>\n",
       "      <td># SQL analytics course with DuckDB - pandas an...</td>\n",
       "      <td>[-0.009578243, -0.011996515, 0.012965736, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SQL analytics course with DuckDB - Sakila BI d...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - Sakila BI d...</td>\n",
       "      <td># SQL analytics course with DuckDB - Sakila BI...</td>\n",
       "      <td>[0.0055291746, -0.004573792, 0.019058505, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SQL analytics course with DuckDB - Sakila BI d...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - Sakila BI d...</td>\n",
       "      <td># SQL analytics course with DuckDB - Sakila BI...</td>\n",
       "      <td>[0.0055291746, -0.004573792, 0.019058505, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SQL analytics course with DuckDB - set theory ...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - set theory ...</td>\n",
       "      <td># SQL analytics course with DuckDB - set theor...</td>\n",
       "      <td>[-0.027204884, -0.023552606, 0.019531347, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>SQL analytics course with DuckDB - set theory ...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - set theory ...</td>\n",
       "      <td># SQL analytics course with DuckDB - set theor...</td>\n",
       "      <td>[-0.027204884, -0.023552606, 0.019531347, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SQL analytics course with DuckDB - setup duckdb</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - setup duckdb</td>\n",
       "      <td># SQL analytics course with DuckDB - setup duc...</td>\n",
       "      <td>[-0.008761382, -0.0034869136, 0.01760935, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SQL analytics course with DuckDB - strings con...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - strings con...</td>\n",
       "      <td># SQL analytics course with DuckDB - strings c...</td>\n",
       "      <td>[-0.02525857, -0.011639317, 0.004399808, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SQL analytics course with DuckDB - strings tut...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - strings tut...</td>\n",
       "      <td># SQL analytics course with DuckDB - strings t...</td>\n",
       "      <td>[-0.021185221, -0.0025146175, 0.01119325, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>SQL analytics course with DuckDB - subquery tu...</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - subquery tu...</td>\n",
       "      <td># SQL analytics course with DuckDB - subquery ...</td>\n",
       "      <td>[-0.012517108, -0.014230472, -0.0019505646, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SQL analytics course with DuckDB - views tutorial</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics course with DuckDB - views tutorial</td>\n",
       "      <td># SQL analytics course with DuckDB - views tut...</td>\n",
       "      <td>[-0.027096331, -0.0073001576, 0.0048975083, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SQL analytics with DuckDB - introduction</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>SQL analytics with DuckDB - introduction</td>\n",
       "      <td># SQL analytics with DuckDB - introduction\\n\\n...</td>\n",
       "      <td>[-0.028552804, -0.0118231885, 0.00639494, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Terraform setup</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>Terraform setup</td>\n",
       "      <td># Terraform setup\\n\\n[00:00:00] Hello and welc...</td>\n",
       "      <td>[-0.004057311, 0.01868715, 0.01563467, -0.0808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>XGBoost hands on tutorial for classification</td>\n",
       "      <td>C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...</td>\n",
       "      <td>XGBoost hands on tutorial for classification</td>\n",
       "      <td># XGBoost hands on tutorial for classification...</td>\n",
       "      <td>[-0.004887973, 0.008207077, 0.013008361, -0.09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               doc_id  \\\n",
       "0      An introduction to the vector database LanceDB   \n",
       "1                                   API trafiklab (1)   \n",
       "2                                       API trafiklab   \n",
       "3               Azure static web app deploy react app   \n",
       "4        Chat with your excel data - xlwings lite (1)   \n",
       "5            Chat with your excel data - xlwings lite   \n",
       "6         Course structure for Azure two weeks course   \n",
       "7                      Data platform course structure   \n",
       "8                   data processing course  structure   \n",
       "9                                   data storytelling   \n",
       "10                             dbt modeling snowflake   \n",
       "11                               docker setup windows   \n",
       "12  FastAPI and scikit-learn API connect to stream...   \n",
       "13                                   Fastapi CRUD app   \n",
       "14                            Hands on regularization   \n",
       "15                                 How does LLM work_   \n",
       "16     Logistic regression hands on with scikit learn   \n",
       "17                         Logistic regression theory   \n",
       "18  Modern data stack - deploy dockerized dashboar...   \n",
       "19   Modern data stack - dockerize your data pipeline   \n",
       "20  Modern data stack - using dlt to extract and l...   \n",
       "21                                Packaging in python   \n",
       "22                                  pandas_read_excel   \n",
       "23                                      postgres sink   \n",
       "24                              Pydantic fundamentals   \n",
       "25  Pydantic with gemini to structure output in a ...   \n",
       "26                                 pydanticAI chatbot   \n",
       "27  PydanticAI fundamentals - outputting structure...   \n",
       "28                                pytest unit testing   \n",
       "29                                Python fundamentals   \n",
       "30                                       python intro   \n",
       "31                                       Python_oop_1   \n",
       "32  Serving PydanticAI Gemini Model with FastAPI T...   \n",
       "33  Serving PydanticAI Gemini Model with FastAPI T...   \n",
       "34  SQL analytics course with DuckDB - course stru...   \n",
       "35  SQL analytics course with DuckDB - CRUD operat...   \n",
       "36  SQL analytics course with DuckDB - dlt to load...   \n",
       "37  SQL analytics course with DuckDB - joins concepts   \n",
       "38  SQL analytics course with DuckDB - joins with ...   \n",
       "39  SQL analytics course with DuckDB - pandas and ...   \n",
       "40  SQL analytics course with DuckDB - pandas and ...   \n",
       "41  SQL analytics course with DuckDB - Sakila BI d...   \n",
       "42  SQL analytics course with DuckDB - Sakila BI d...   \n",
       "43  SQL analytics course with DuckDB - set theory ...   \n",
       "44  SQL analytics course with DuckDB - set theory ...   \n",
       "45    SQL analytics course with DuckDB - setup duckdb   \n",
       "46  SQL analytics course with DuckDB - strings con...   \n",
       "47  SQL analytics course with DuckDB - strings tut...   \n",
       "48  SQL analytics course with DuckDB - subquery tu...   \n",
       "49  SQL analytics course with DuckDB - views tutorial   \n",
       "50           SQL analytics with DuckDB - introduction   \n",
       "51                                    Terraform setup   \n",
       "52       XGBoost hands on tutorial for classification   \n",
       "\n",
       "                                             filepath  \\\n",
       "0   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "1   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "2   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "3   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "4   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "5   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "6   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "7   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "8   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "9   C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "10  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "11  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "12  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "13  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "14  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "15  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "16  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "17  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "18  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "19  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "20  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "21  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "22  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "23  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "24  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "25  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "26  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "27  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "28  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "29  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "30  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "31  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "32  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "33  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "34  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "35  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "36  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "37  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "38  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "39  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "40  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "41  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "42  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "43  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "44  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "45  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "46  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "47  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "48  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "49  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "50  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "51  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "52  C:\\Users\\eriku\\Desktop\\DataenginerSTI2024-2026...   \n",
       "\n",
       "                                             filename  \\\n",
       "0      An introduction to the vector database LanceDB   \n",
       "1                                   API trafiklab (1)   \n",
       "2                                       API trafiklab   \n",
       "3               Azure static web app deploy react app   \n",
       "4        Chat with your excel data - xlwings lite (1)   \n",
       "5            Chat with your excel data - xlwings lite   \n",
       "6         Course structure for Azure two weeks course   \n",
       "7                      Data platform course structure   \n",
       "8                   data processing course  structure   \n",
       "9                                   data storytelling   \n",
       "10                             dbt modeling snowflake   \n",
       "11                               docker setup windows   \n",
       "12  FastAPI and scikit-learn API connect to stream...   \n",
       "13                                   Fastapi CRUD app   \n",
       "14                            Hands on regularization   \n",
       "15                                 How does LLM work_   \n",
       "16     Logistic regression hands on with scikit learn   \n",
       "17                         Logistic regression theory   \n",
       "18  Modern data stack - deploy dockerized dashboar...   \n",
       "19   Modern data stack - dockerize your data pipeline   \n",
       "20  Modern data stack - using dlt to extract and l...   \n",
       "21                                Packaging in python   \n",
       "22                                  pandas_read_excel   \n",
       "23                                      postgres sink   \n",
       "24                              Pydantic fundamentals   \n",
       "25  Pydantic with gemini to structure output in a ...   \n",
       "26                                 pydanticAI chatbot   \n",
       "27  PydanticAI fundamentals - outputting structure...   \n",
       "28                                pytest unit testing   \n",
       "29                                Python fundamentals   \n",
       "30                                       python intro   \n",
       "31                                       Python_oop_1   \n",
       "32  Serving PydanticAI Gemini Model with FastAPI T...   \n",
       "33  Serving PydanticAI Gemini Model with FastAPI T...   \n",
       "34  SQL analytics course with DuckDB - course stru...   \n",
       "35  SQL analytics course with DuckDB - CRUD operat...   \n",
       "36  SQL analytics course with DuckDB - dlt to load...   \n",
       "37  SQL analytics course with DuckDB - joins concepts   \n",
       "38  SQL analytics course with DuckDB - joins with ...   \n",
       "39  SQL analytics course with DuckDB - pandas and ...   \n",
       "40  SQL analytics course with DuckDB - pandas and ...   \n",
       "41  SQL analytics course with DuckDB - Sakila BI d...   \n",
       "42  SQL analytics course with DuckDB - Sakila BI d...   \n",
       "43  SQL analytics course with DuckDB - set theory ...   \n",
       "44  SQL analytics course with DuckDB - set theory ...   \n",
       "45    SQL analytics course with DuckDB - setup duckdb   \n",
       "46  SQL analytics course with DuckDB - strings con...   \n",
       "47  SQL analytics course with DuckDB - strings tut...   \n",
       "48  SQL analytics course with DuckDB - subquery tu...   \n",
       "49  SQL analytics course with DuckDB - views tutorial   \n",
       "50           SQL analytics with DuckDB - introduction   \n",
       "51                                    Terraform setup   \n",
       "52       XGBoost hands on tutorial for classification   \n",
       "\n",
       "                                              content  \\\n",
       "0   # An introduction to the vector database Lance...   \n",
       "1   # API trafiklab\\n\\n[00:00:00] Hello and welcom...   \n",
       "2   # API trafiklab\\n\\n[00:00:00] Hello and welcom...   \n",
       "3   # Azure static web app deploy react app\\n\\n[00...   \n",
       "4   # Chat with your excel data - xlwings lite\\n\\n...   \n",
       "5   # Chat with your excel data - xlwings lite\\n\\n...   \n",
       "6   # Course structure for Azure two weeks course\\...   \n",
       "7   # Data platform course structure\\n\\n[00:00:00]...   \n",
       "8   # data processing course structure\\n\\n**Kokchu...   \n",
       "9   # data storytelling\\n\\n[00:00:00] Hello and we...   \n",
       "10  # dbt modeling snowflake\\n\\n**Kokchun Giang:**...   \n",
       "11  # docker setup windows\\n\\n[00:00:00] Hello and...   \n",
       "12  # FastAPI and scikit-learn API connect to stre...   \n",
       "13  # Fastapi CRUD app\\n\\n**Kokchun Giang:** [00:0...   \n",
       "14  # Hands on regularization\\n\\n[00:00:00] Hello ...   \n",
       "15  # How does LLM work?\\n\\n[00:00:00] Hello and w...   \n",
       "16  # Logistic regression hands on with scikit lea...   \n",
       "17  # Logistic regression theory\\n\\n[00:00:00] Hel...   \n",
       "18  # Modern data stack - deploy dockerized dashbo...   \n",
       "19  # Modern data stack - dockerize your data pipe...   \n",
       "20  # Modern data stack - using dlt to extract and...   \n",
       "21  # Packaging in python\\n\\n[00:00:00] Hello and ...   \n",
       "22  # pandas\\_read\\_excel\\n\\n[00:00:00] Hello and ...   \n",
       "23  # postgres sink\\n\\n[00:00:00] Hello and welcom...   \n",
       "24  # Pydantic fundamentals\\n\\n[00:00:00] Hello an...   \n",
       "25  # Pydantic with gemini to structure output in ...   \n",
       "26  # pydanticAI chatbot\\n\\n[00:00:00] Hello and w...   \n",
       "27  # PydanticAI fundamentals - outputting structu...   \n",
       "28  # pytest unit testing\\n\\n[00:00:00] Hello and ...   \n",
       "29  # Python fundamentals\\n\\n[00:00:00] Hello and ...   \n",
       "30  # python intro\\n\\n[00:00:00] Hello and welcome...   \n",
       "31  # Python\\_oop\\_1\\n\\n**kokchun:** [00:00:00] He...   \n",
       "32  # Serving PydanticAI Gemini Model with FastAPI...   \n",
       "33  # Serving PydanticAI Gemini Model with FastAPI...   \n",
       "34  # SQL analytics course with DuckDB - course st...   \n",
       "35  # SQL analytics course with DuckDB - CRUD oper...   \n",
       "36  # SQL analytics course with DuckDB - dlt to lo...   \n",
       "37  # SQL analytics course with DuckDB - joins con...   \n",
       "38  # SQL analytics course with DuckDB - joins wit...   \n",
       "39  # SQL analytics course with DuckDB - pandas an...   \n",
       "40  # SQL analytics course with DuckDB - pandas an...   \n",
       "41  # SQL analytics course with DuckDB - Sakila BI...   \n",
       "42  # SQL analytics course with DuckDB - Sakila BI...   \n",
       "43  # SQL analytics course with DuckDB - set theor...   \n",
       "44  # SQL analytics course with DuckDB - set theor...   \n",
       "45  # SQL analytics course with DuckDB - setup duc...   \n",
       "46  # SQL analytics course with DuckDB - strings c...   \n",
       "47  # SQL analytics course with DuckDB - strings t...   \n",
       "48  # SQL analytics course with DuckDB - subquery ...   \n",
       "49  # SQL analytics course with DuckDB - views tut...   \n",
       "50  # SQL analytics with DuckDB - introduction\\n\\n...   \n",
       "51  # Terraform setup\\n\\n[00:00:00] Hello and welc...   \n",
       "52  # XGBoost hands on tutorial for classification...   \n",
       "\n",
       "                                            embedding  \n",
       "0   [-0.038686633, 0.0036908067, 0.02178414, -0.07...  \n",
       "1   [-0.0066460855, -0.0048297336, 0.014259387, -0...  \n",
       "2   [-0.0066460855, -0.0048297336, 0.014259387, -0...  \n",
       "3   [-0.012295628, 0.015832268, 0.025340993, -0.09...  \n",
       "4   [-0.0071265996, 0.020476552, 0.017235534, -0.0...  \n",
       "5   [-0.0071265996, 0.020476552, 0.017235534, -0.0...  \n",
       "6   [0.0015313567, 0.012794174, 0.016358217, -0.07...  \n",
       "7   [0.010435624, -0.0016020014, 0.011163727, -0.0...  \n",
       "8   [0.0032914313, 0.01059015, 0.009196502, -0.064...  \n",
       "9   [-0.011316549, 0.016874935, 0.012392512, -0.08...  \n",
       "10  [-0.010636117, 0.01212832, 0.029030465, -0.086...  \n",
       "11  [-0.0034482135, 0.010729573, 0.016939094, -0.0...  \n",
       "12  [-0.017560659, 0.009985835, 0.01749353, -0.085...  \n",
       "13  [-0.012556567, -0.01593715, 0.022671303, -0.07...  \n",
       "14  [-0.009709013, 6.357031e-05, 0.023912106, -0.0...  \n",
       "15  [-0.027908303, 0.017137818, 0.023856219, -0.06...  \n",
       "16  [-0.0066825696, 0.002244388, 0.011106265, -0.0...  \n",
       "17  [-0.008146983, -0.00016716555, 0.011266027, -0...  \n",
       "18  [-0.0075815883, 0.02116889, 0.027429571, -0.10...  \n",
       "19  [0.0027987233, 0.014228618, 0.027441906, -0.09...  \n",
       "20  [-0.00826649, 0.019363934, 0.016988434, -0.080...  \n",
       "21  [-0.0150246415, 0.0042777252, 0.038052212, -0....  \n",
       "22  [-0.0038475876, 0.0054737586, 0.024301918, -0....  \n",
       "23  [0.007228276, 0.021961471, 0.031665433, -0.088...  \n",
       "24  [-0.017777685, -0.01204192, 0.02006065, -0.059...  \n",
       "25  [-0.01512796, -0.003907627, 0.015176425, -0.07...  \n",
       "26  [-0.012143856, -0.001205422, 0.013563879, -0.0...  \n",
       "27  [-0.033087507, 0.0036856267, 0.02514256, -0.07...  \n",
       "28  [-0.014343338, 0.0007622975, 0.030120881, -0.0...  \n",
       "29  [-0.011976539, 0.0025688808, 0.015519834, -0.0...  \n",
       "30  [-0.0110368235, -0.0020397531, 0.012955041, -0...  \n",
       "31  [-0.008043373, -0.011165032, 0.031197485, -0.0...  \n",
       "32  [-0.018388461, -0.006911759, 0.015190295, -0.0...  \n",
       "33  [-0.018388461, -0.006911759, 0.015190295, -0.0...  \n",
       "34  [0.0023245383, -0.0029253534, 0.0036489798, -0...  \n",
       "35  [-0.018357037, -0.0009783215, 0.025988214, -0....  \n",
       "36  [-0.0066205882, -0.0075128363, 0.019882608, -0...  \n",
       "37  [-0.027722066, -0.016966412, 0.012043696, -0.0...  \n",
       "38  [-0.022949534, -0.022863599, 0.013395227, -0.0...  \n",
       "39  [-0.009578243, -0.011996515, 0.012965736, -0.0...  \n",
       "40  [-0.009578243, -0.011996515, 0.012965736, -0.0...  \n",
       "41  [0.0055291746, -0.004573792, 0.019058505, -0.0...  \n",
       "42  [0.0055291746, -0.004573792, 0.019058505, -0.0...  \n",
       "43  [-0.027204884, -0.023552606, 0.019531347, -0.0...  \n",
       "44  [-0.027204884, -0.023552606, 0.019531347, -0.0...  \n",
       "45  [-0.008761382, -0.0034869136, 0.01760935, -0.0...  \n",
       "46  [-0.02525857, -0.011639317, 0.004399808, -0.07...  \n",
       "47  [-0.021185221, -0.0025146175, 0.01119325, -0.0...  \n",
       "48  [-0.012517108, -0.014230472, -0.0019505646, -0...  \n",
       "49  [-0.027096331, -0.0073001576, 0.0048975083, -0...  \n",
       "50  [-0.028552804, -0.0118231885, 0.00639494, -0.0...  \n",
       "51  [-0.004057311, 0.01868715, 0.01563467, -0.0808...  \n",
       "52  [-0.004887973, 0.008207077, 0.013008361, -0.09...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"articles\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1edb4d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# SQL analytics with DuckDB - introduction\\n\\n[00:00:00] Hello and welcome to this lecture in SQL introduction and we'll go into the theory about the introduction to SQL and using ~~D ~~DB in SQL. This is an introductory lecture in this course SQL analytics with ~~D ~~db. You will learn a lot about how. To use ~~D ~~DB to turn metadata into valuable insights.\\n\\nThat is the main topic for this course. And later on in this course, we'll also connect to Python that you can work with DB within the Python ecosystem and within the data science, data engineering, data analytics ecosystem. This is really fun and we'll go directly to the slides to get an introduction to what, sQL is.\\n\\n**Kokchun Giang-2:** Turning data into valuable insights using SQL. We start with an example, an ice cream startup. [00:01:00] Swedish glass is using Excel sheets to store data. This might be ~~a sound like ~~a viable solution ~~in the beginning, and it is ~~in the beginning. You just store the data in Excel sheets. We have orders, we store them in Excel sheets.\\n\\nWe have customers, we store them in Excel sheets, we have inventory. We store them in Excel sheets. Okay. It sounds quite simple and yeah, we can start like this. It's no problems ~~in the beginning. Okay. ~~In the beginning. But the company grew. This was really a classic, like people really like this ice cream.\\n\\nThey ordered a lot. And we got a lot of customers. And then when we did that, we needed to employ more people. The thing is, we have a team now, each Excel file is shared across the team. We have orders, we have customers, we have inventory. ~~And also a lot more, of course, ~~when the business is growing.\\n\\nBut let's simplify to this. We have [00:02:00] one person that is working with this. We have another one that is working with this. We have a third one that is working with this and a fourth one. ~~Okay. ~~If there's a lot of people working with these Excel sheets and we get a lot of data, then it's quite easy to get duplicates, for example.\\n\\nCommon problems that ~~are ~~arose when sharing Excel sheets in this way, we get data duplication. Accidentally ~~we create ~~creating duplicates without knowing ~~about ~~that these lines may have existed elsewhere. ~~That is ~~and that is the problem that we need to manually update in several places.\\n\\nAnd we get inconsistent data. For example, we look for one person and then suddenly there is a similar person with different addresses or same person with different addresses. Then we have an inconsistency in addresses, for example. One is [00:03:00] a typo. Another one is not we have relationships.\\n\\nIt's hard to manage manually between customers and orders, which customer connects to which orders that is hard to manage manually. Team, they manually links the order to the right customer. This takes a lot of time and it's manual effort, and it will lead to inconsistencies and bugs like you, you do errors because humans it's human is not good at manual work performance.\\n\\nOur Excel sheet is growing larger and larger, and this will cause performance issues. It'll get slow ~~to ~~to work with huge amounts of data in Excel and many more problems that we won't go into in this slide here. The idea is that we need something else ~~to ~~to manage this when our company's [00:04:00] growing.\\n\\nAn example of inconsistent data. You can see it here due to manual input. For example, here you have Alice Frost, alice@example.com and this address, and then you have Alice f alice\\\\_f@example.com. Okay? It seems to be the same person or same phone number, same address. But here we have LSF, here we have Alice Frost.\\n\\nHere are Alice F at example. Here we have Alice at example. Is it the same person or not? That is a question. Is this the same person? This is and also we will come back more into how to handle inconsistent data and how to. Make sure that the data is unique and what is determining a unique row, et cetera.\\n\\nThis will come back more into a data modeling course later on. However, for ~~this in ~~this course, we'll focus a lot in sk l queries and how to analyze the data. ~~And ~~this is due [00:05:00] to being able to both learn. Fundamentals in SQL and to work with it with analytics and for transactional later on.\\n\\nAnd also we will learn it ~~for ~~that you can go~~ on ~~for example, ~~go ~~towards data warehouses and you do need to understand how to analyze the data. But that was a side note on what I just said. Now, which ~~Alice ~~is linked to what orders in the orders table. Using relational databases and SQL, we can handle many of these issues.\\n\\nSQL is a structured query language ~~and we work, ~~and the database is a relational database, there are also non relational databases such as a document database, vector database, et cetera. And no SQL. But here we have relational databases and SQL, they follow something called the relational model that will come back later on as [00:06:00] well.\\n\\nDefine relationships in SQL tables. ~~If ~~this ensures data consistency. ~~Op, ~~they're optimized for large volumes of data, scalable and efficient for querying of data. Data constraints, like data types and unique values ~~are ~~are put in ~~that they ~~make sure that the data conforms to a specific type.\\n\\nFor example, you have a numeric column, then you cannot put in strings or ~~other ~~other types of data. ~~And ~~and you have unique values. You can make unique constraints. This makes sure that we get automatic validation, which reduces error. I talk about this type of thing.\\n\\nThis part is more concern with something called OLTP that will come back to more transactional databases. And there's a lot of ~~more more ~~nice benefits ~~with ~~with a database. SKL, what is this? [00:07:00] What is SGL is structured query language. We have create, update, and we organize data into tables.\\n\\nOkay? Tables, we have rows and columns. You can see it is similar to the Excel sheets. You have rows and columns, right? And SGL has standardized language. It's standardized. There, there is a lot of different flavors of SQL, but mostly it's similar. And ~~there, ~~there's some syntax that differs.\\n\\nBut the foundation is the same. It's based on a relational model. And what this means is that we organize data into related tables. We link the data through unique identifiers. ~~What ~~if you don't understand what this means right now, it's fine. And we'll come back to this ~~term ~~terminology later on.\\n\\nThere's some core functions within SQL. [00:08:00] We get DDL data definition language.\\n\\n**Kokchun Giang-3:** define structure of database, for example tables and what type of data it holds. We use the crate alter and drop, for example. We can create table, we can alter the table, we can drop tables. This is DDL, data, definition language. Moving on, ~~we have let's see. Oh, sorry. ~~We have data manipulation language, DML, these are to manipulate data directly in the database.\\n\\nwe have insert command, we have update command, we have delete command,\\n\\nand then we have DQL data query language. It's to select to you, you use select statement to retrieve a specific information from a database. Letting users access and fill the data for insights. We use the select statement and or the select class.\\n\\nWe'll see. We will use it very a [00:09:00] lot in this course to analyze the data and we use it together with a lot of other things to filter. For example, the wear class, the group buy, et cetera.\\n\\nAnd then we have DCL data control language. We have here you can revoke and grant to give access to other uses.\\n\\nYou, you may want to give access to certain uses for certain type, certain database or certain tables, right? Depending on which SQL you use there's a possibility for more or less. DCL. Yes. And then we go into the SQL flavor that we will use in this course we'll use Duck db.\\n\\nMeet Duck db a modern, powerful database management system for analytics. It's super powerful, it's super performant. ~~And also it's something called oap. Opt, ~~it's optimized for intensive analytical [00:10:00] queries that you can do locally in your own computer. And it's very easy to set up. ~~It's it's just a, ~~you just use a ~~file, a duct ~~DB file, and then you can connect to it in the terminal.\\n\\nAnd also we talked about Ola. Then we need to mention OLTP, which is for transactional database. That is optimized for a lot of inserts into the database. For example, other ~~SGL other ~~RD. Such as Postgres, ~~SGL, ~~Microsoft ~~sgl ~~they use and they are ~~OLCP, ~~right?\\n\\nWhile ~~Dtb, RO ~~is Ola. It's very highly performant, on your own machine and can handle large data sets. It's embedded database, no need for separate server and database is contained in the file. This makes it very simple to work with and very good to start with as a as a first course within ~~SKL ~~analytics.\\n\\nTo just learn how to get insights from the data, [00:11:00] to filter the data, to manipulate it in the way that you want. And it integrates very well with other tools in the data science ecosystem or data engineering, data analytics ecosystem such as Python. Pandas and data frames. We will work with it together with pandas in this course that you can see the power of combining Python and ~~a scale combining Python and Duct ~~DB in order ~~to ~~to get data insights that you want.\\n\\nVery good for data analysis, can run complex queries for analytics and reporting. And it's very good for building data transformations in an ETL pipeline to serve business intelligence and ai. We'll come back ~~in ~~in more if you follow me, ~~we have a lot of, ~~I have courses in ~~data within ~~data engineering, a lot of them.\\n\\nAnd then there I usually build ~~pipelines ~~ETL or ELT pipelines to be more. Specific. ~~And ~~and there DDB is a core part. Could be a core [00:12:00] part of it where you store the data. You could use DDB as the data warehouse, for example. A data engineering pipeline with an OLA database as a data warehouse.\\n\\nThe reason why I picked data engineering pipeline is that when you're working with data analytics, you usually~~ you ~~have the data. You want the data to go through different types of transformations. Here's~~ engineering pipe, ~~data, engineering pipeline, and we use for example, DDB as the data warehouse.\\n\\nBut you could use something else, ~~a cloud, ~~such as snowflake, ~~for example for that but or ~~Amazon, Redshift ~~or some, ~~or Google BigQuery. But~~ but TDB ~~could work for local solution. What you have is that you have different data sources. ~~This ~~this could be like CSV files ~~here. It could have like ~~APIs.\\n\\nYou could have other types of data. And they are ingested into a data warehouse. The data warehouse could be this DDB file, for example. And then you serve dashboard. You [00:13:00] serve machine learning models. Then you need to transform the data ~~into certain way, ~~certain formats ~~that ~~they can be served ~~to this downstream ~~downstream.\\n\\nApplications. Here you have ELT. Extract and load. You extract and load the data into your data warehouse. Then you transform it inside your data warehouse, and then you serve the data for the end users. And the dashboards and the ML models. Here, for example, other people can take over.\\n\\nFor example, the BI analyst could take over here could be like a machine learning engineer or a data scientist that take over and~~ and and take ~~take the data from the data warehouse. Ddb, it could work as a lightweight data warehouse for small to medium sized. Data. ~~And if you ~~and if you grow out of your DDB database that it means that you cannot do it locally anymore, then you can consider ~~cloud ~~cloud solutions.\\n\\nBut they ~~cost the ~~cost money. Okay? Yes.\\n\\nthat was an [00:14:00] introduction to ~~sq, ~~SQL and DDB ~~in ~~and kind of introduction to this course as well. But we'll have more introductions such as the core structure that you can see what is the actual content of this course. And, who this is suitable for. ~~And here but ~~I hope that you have learned some basic concepts about the motivation, about ~~the ~~why to use SQL and ~~the ~~databases and then also going into ~~like ~~DDB and some motivation about that.\\n\\nStay attuned for more videos where we'll actually go into coding and more practical development and analyzing of data. And that will be really fun. I hope to see you there and thanks ~~to what ~~for watching this video and see you in the next one ~~by, I.~~\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"articles\"].search(\"SQL analytics\").limit(3).to_list()[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a4c173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12482"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = vector_db[\"articles\"].search(\"SQL analytics\").limit(3).to_list()[0][\"content\"]\n",
    "\n",
    "len(length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
